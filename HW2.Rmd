---
title: "HW2"
author: "Shruti Kapur, Immanuel Ponminissery, Patricia Schutter, Dian Zhao"
date: "08/17/2020"
output:
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_knit$set(echo = TRUE)
webshot::install_phantomjs()
```

```{r, echo=FALSE}
rm(list = ls())
library(mosaic)
library(tidyverse)
library(ggplot2)
library(scales)
options(scipen = 999999999) ### turn off scientific notation like 1e-09

```
#Problem 1

```{r, echo=FALSE}
greenbuilding = read.csv('data/greenbuildings.csv')

attach(greenbuilding)

greenbuilding$cluster <- as.factor(greenbuilding$cluster)
greenbuilding$renovated <- as.factor(greenbuilding$renovated)
greenbuilding$class_a <- as.factor(greenbuilding$class_a)
greenbuilding$class_b <- as.factor(greenbuilding$class_b)
greenbuilding$green_rating <- as.factor(greenbuilding$green_rating)
greenbuilding$LEED <- as.factor(greenbuilding$LEED)
greenbuilding$Energystar <- as.factor(greenbuilding$Energystar)
greenbuilding$net <- as.factor(greenbuilding$net)
greenbuilding$amenities <- as.factor(greenbuilding$amenities)

greenbuilding$class <- ifelse(class_a == 1 & class_b == 0, 'class a',
                              ifelse(class_a == 0 & class_b == 1, 
                              'class b','class c'))
n=nrow(greenbuilding)
greenbuilding$green<- ifelse(green_rating == 1, 'Green Building', 
                             'Nongreen Building')
```

Basic information about our building:
  Total GFA: 250,000 sqft
  Storie: 15 
  Age: 0
  Baseline Construction Cost per sqft: $400
  
The question we are trying to answer is whether buildings with green certificates can commnad higher rent, with *everything else being equal*. So the key is to use a comparable set of buildings data to estimate rents for our building, both with and without green certificates. 

There are lots of factors that influence average rents. The stats guru only accounted for leasing rate and outliers by getting rid of the buildings with leasing_rate < 10% and use median rent instead of mean rent. However, he did not account for other factors that may skew the comparison between green buildings and non-green buildings. 

For example, the following histogram shows that nearlly all of the green buildings are less than 50 years but a significant amount of nongreen buildings are more than 50 years old. Since newer buildings generally charge higher rents, the rent differential between green and nongreen buildings may be the result of the age of the building, and not necessarily due to the green certificate. 
```{r}
ggplot(greenbuilding,aes(x=age)) + 
  geom_histogram(bins = 30,color="black", fill="white") +
  facet_wrap(~ green,nrow = 1)
```

In addition, only one third of the nongreen buildings are class a buildings whereas more than three quarters of green buildings are class a building. Thus the higher rent charged by green buildings could also be explained by building quality. 
```{r}
ggplot(greenbuilding) +
  geom_col(aes(x = 1, y = n, fill = class), position = "fill") +
  coord_polar(theta = "y") +
  facet_wrap(~ green) +
    theme_bw() +
    theme(axis.title = element_blank(),
          axis.text = element_blank(),
          axis.ticks = element_blank(),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          panel.border = element_blank())
```

Green buildings also tend to have better amenities. 
```{r}
ggplot(greenbuilding) +
  geom_col(aes(x = 1, y = n, fill = amenities), position = "fill") +
  coord_polar(theta = "y") +
  facet_wrap(~ green) +
    theme_bw() +
    theme(axis.title = element_blank(),
          axis.text = element_blank(),
          axis.ticks = element_blank(),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          panel.border = element_blank()) +
  labs(
    caption = "Amenities = 1 means with amenities and 0 means without"
  )
```

Lastly, we have to account for location. If green buildings are generally built in areas with higher rent, then the rent differential between green and nongreen buildings are not due to the green certificate, but more likely due to location premium.

Thus, we have to compare rent differentials between buildings in the same location. We first create a compare_set that only include buildings with leasing rate more than 10%, less than 50 years old, with class a ratings and have amenities. 

```{r}
compare_set <- subset(greenbuilding, leasing_rate > 10 &
                      age < 50 & class_a == 1 & amenities == 1)

```

We then create two subsets for green and nongreen buildings. For each subset, we calculate the median rent for each cluster. We then create a separate data table with each cluster as one row and with three columns: median rent for green buildings, median rent for nongreen buildings and rent differentials between green and nongreen buildings. 
```{r}
green <- compare_set[compare_set$green_rating == 1, ]
nongreen <- compare_set[compare_set$green_rating == 0, ]

green_median = green %>%
  group_by(cluster) %>%
  summarize(median_rent = median(Rent))

nongreen_median = nongreen %>%
  group_by(cluster) %>%
  summarize(median_rent = median(Rent))

merged = merge(nongreen_median,green_median,by.x = 'cluster', by.y = 'cluster',suffixes  = c(".nongreen", ".green"))

merged$rent_diff = merged$median_rent.green-merged$median_rent.nongreen
```

The scatterplot shows that the rent differentials are equally spread along the 0 line. The average rent differential is -0.123, suggesting that green buildings do not in fact command a higher rent compared to non green buildings. 

```{r}
ggplot(data = merged) + 
  geom_point(mapping = aes(x = cluster, y = rent_diff)) +
  geom_hline(aes(yintercept=0),
            color="blue", linetype="dashed", size=1) +
  annotate("text", x=20, y=20, color="red", label= paste('avg rent differential: ', mean(merged$rent_diff)),hjust=0) 
```

In conclusion, our analysis shows that green certificates do not generate rent premium. The higher rent charged by green buildings are likely the result of other factors such as newer buildings and better amenities. Thus, from a purely economic perspective, it is not advisable to get green certified. 

#Problem 2
The following is an analysis of Austin Bergstrom International Airport. The data
has been grouped in a way that shows optimal times for flying in regards to 
month, day of week, time of day, and carrier in order to avoid arrival and/or
departure delays.

```{r, echo=TRUE}
library(mosaic)
library(tidyverse)
library(ggplot2)

data1 <-"data/ABIA.csv"
ABIA = read.csv(data1)
#ABIA = read.csv('ABIA.csv')
head(ABIA)

# Delays vs. Month
plot(ABIA$Month, ABIA$ArrDelay, main = "Arrival Delays Occur Most in Dec. and March", xlab= "Month", ylab= "Arrival Delay")
```
This is showing the arrival delays per month. From this, it appears that most arrival delays occur in December and March.
```{r, echo=TRUE}

plot(ABIA$Month, ABIA$DepDelay, main = "Departure Delay Occur Most in March and August", xlab= "Month", ylab= "Departure Delay")
```
To compare arrival delays with depature delays, departure delays were aslo ran against months. Departure delays occur most in March and August. As March was present in both results, March seems to be full of delays of both kinds.
```{r, echo=TRUE}
# Delays vs. Day of Week
plot(ABIA$DayOfWeek, ABIA$ArrDelay, main = "Tuesday Has Lowest Arrival Delay", xlab= "Day of Week", ylab= "Arrival Delay")
plot(ABIA$DayOfWeek, ABIA$DepDelay, main = "Tuesday Has Lowest Departure Delay", xlab= "Day of Week", ylab= "Departure Delay")
```
These are a couple plots that represent what day is best to fly on in order to avoid delays. Tuesday was found as the best day to fly in order to avoid delays.

```{r, echo=TRUE}

# Departure Time vs. Delay
ggplot(data = ABIA) + 
  geom_point(mapping = aes(x = DepTime, y = DepDelay)) +
  ggtitle("Later Departure Flights Have Higher Delays")

# Arrival Time vs. Delay
ggplot(data = ABIA) + 
  geom_point(mapping = aes(x = ArrTime, y = ArrDelay)) +
  ggtitle("      Arrival Delays Greatest At Night")
```
Now that month and day have been narrowed down, the next step was to narrow down a time to fly. From these plots, it appears that mornings are best to fly. However this does not mean morning as in midnight, but rather around 5AM. The increase around midnight is rollover from the high level of delays as it gets later in the night.

```{r, echo=TRUE}
################################################

# Departure Delay vs. Distance
ggplot(data = ABIA) + 
  geom_point(mapping = aes(x = DepDelay, y = Distance, color = Distance)) + 
  ggtitle("Departure Delay Greatest For Small and Large Distances")

# Arrival Delay vs. Distance
ggplot(data = ABIA) + 
  geom_point(mapping = aes(x = ArrDelay, y = Distance, color = Distance)) + 
  ggtitle("Arrival Delay Greatest For Small and Large Distances")
```
Next, I wanted to see whether distance was a factor in delays. From these plots, we can see that delays are greatest both on the extremes. This makes sense as shorter flights don't have enough distance to make up for lost time and long distances tend to require more preparations.

```{r, echo=TRUE}
# Arrival Delay vs. Carrier
ggplot(data = ABIA) + 
  geom_point(mapping = aes(x = ArrDelay, y = UniqueCarrier, color = UniqueCarrier)) + 
  ggtitle("Arrival Delay Greatest Among JetBlue and SouthWest")

# Departure Delay vs. Carrier
ggplot(data = ABIA) + 
  geom_point(mapping = aes(x = DepDelay, y = UniqueCarrier, color = UniqueCarrier)) + 
  ggtitle("Departure Delay Greatest Among JetBlue")
```
These plots are testing whether a specific carrier tends to have more delays than others. Both Jetblue and SouthWest tend to have the most arrival delays, while JetBlue also seems to have the greatest departure delays. To avoid delays, one might want to fly with someone other than JetBlue.

```{r, echo=TRUE}

###
# facets
###

#####################################################################
# Departure Delay vs. Day of Week vs. Month
ggplot(data = ABIA) + 
  geom_point(mapping = aes(x = DepDelay, y = DayOfWeek)) + 
  facet_wrap(~ Month, nrow = 2) +
  ggtitle("What Day to Fly, Per Month to Avoid Departure Delays")

# Arrival Delay vs. Day of Week vs. Month
ggplot(data = ABIA) + 
  geom_point(mapping = aes(x = ArrDelay, y = DayOfWeek)) + 
  facet_wrap(~ Month, nrow = 2) +
  ggtitle("What Day to Fly, Per Month to Avoid Arrival Delays")
###################################################################33
```
In preparation for your next flight, you might want to know what a certain day looks like on the particular month you need to travel. These two plots will point you in the right direction, where you will first be able to look at the month that you are flying and then find the smallest line, indicating that is the day with the fewest delays during that month.

#Problem 3
```{r, echo=TRUE}
library(mosaic)
library(quantmod)
library(foreach)

#### Possibility 1
#### XLE is an energy ETF, XLI is an industrial ETF, XLP is a consumer staples ETF
#### This is a homogeneous portfolio

mystocks = c("XLE", "XLI", "XLP")
myprices = getSymbols(mystocks, from = "2014-01-01")


#### Adjusting Stocks
for(ticker in mystocks) {
  expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
  eval(parse(text=expr))
}

# Combine all the returns in a matrix
all_returns = cbind(	ClCl(XLEa),
                     ClCl(XLIa),
                     ClCl(XLPa))
all_returns = as.matrix(na.omit(all_returns))

# Compute the returns from the closing prices
pairs(all_returns)

# Sample a random return from the empirical joint distribution
return.today = resample(all_returns, 1, orig.ids=FALSE)

# Update the value of your holdings, assuming equal allocation
total_wealth = 100000
my_weights = c(0.34,0.33,0.33)
holdings = total_wealth*my_weights
holdings = holdings*(1 + return.today)

# Compute new total wealth
holdings
total_wealth = sum(holdings)
total_wealth


# Now simulate many different possible futures
initial_wealth = 100000
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
  total_wealth = initial_wealth
  weights = c(0.34,0.33,0.33)
  holdings = weights * total_wealth
  n_days = 20
  wealthtracker = rep(0, n_days)
  for(today in 1:n_days) {
    return.today = resample(all_returns, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
  }
  wealthtracker
}

# each row is a simulated trajectory
# each column is a data
head(sim1)
hist(sim1[,n_days], 25)

# Profit/loss
mean(sim1[,n_days])
mean(sim1[,n_days] - initial_wealth)
hist(sim1[,n_days]- initial_wealth, breaks=30)

# 5% value at risk:
quantile(sim1[,n_days]- initial_wealth, prob=0.05)

```
 Portfolio 1 is a small and homogeneous portfolio containng : XLE - an energy ETF, XLI - an industrial ETF, XXP - a consumer staples ETF. We see a nearly normal distribution, with the returns skewed slightly towards the negative side.
 
```{r, echo=TRUE}

#### Possibility 2
#### VWO is an Asia Pacific ETF, GCX is a China ETF, EZU is an Europe ETF, EWJ is a Japan ETF, EWZ is a latin America ETF
#### This is a globally hedged portfolio

mystocks = c("VWO", "GCX", "EZU", "EWJ", "EWZ")
myprices = getSymbols(mystocks, from = "2014-01-01")


#### Adjusting Stocks
for(ticker in mystocks) {
  expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
  eval(parse(text=expr))
}

# Combine all the returns in a matrix
all_returns = cbind(	ClCl(VWOa),
                     ClCl(GCXa),
                     ClCl(EZUa),
                     ClCl(EWJa),
                     ClCl(EWZa))
all_returns = as.matrix(na.omit(all_returns))

# Compute the returns from the closing prices
pairs(all_returns)

# Sample a random return from the empirical joint distribution
return.today = resample(all_returns, 1, orig.ids=FALSE)

# Update the value of your holdings, assuming equal allocation
total_wealth = 100000
my_weights = c(0.2,0.2,0.2,0.2,0.2)
holdings = total_wealth*my_weights
holdings = holdings*(1 + return.today)

# Compute new total wealth
holdings
total_wealth = sum(holdings)
total_wealth


# Now simulate many different possible futures
initial_wealth = 100000
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
  total_wealth = initial_wealth
  weights = c(0.2,0.2,0.2,0.2,0.2)
  holdings = weights * total_wealth
  n_days = 20
  wealthtracker = rep(0, n_days)
  for(today in 1:n_days) {
    return.today = resample(all_returns, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
  }
  wealthtracker
}

# each row is a simulated trajectory
# each column is a data
head(sim1)
hist(sim1[,n_days], 25)

# Profit/loss
mean(sim1[,n_days])
mean(sim1[,n_days] - initial_wealth)
hist(sim1[,n_days]- initial_wealth, breaks=30)

# 5% value at risk:
quantile(sim1[,n_days]- initial_wealth, prob=0.05)
```
Portfolio 2 is a globally hedged portfolio, with holdings across geographies.  VWO is an Asia Pacific ETF, GCX is a China ETF, EZU is an Europe ETF, EWJ is a Japan ETF, EWZ is a latin America ETF. We see the simulation giving us a positively skewed return over time. This is logical, given the geographic hedging. However, it is worthwhile to note that the frequency is highest for the 0 to slightly negative bin (almost -10,000).
```{r}
#### Possibility 3
#### SPY is a large cap growth ETF, SHY is a short treasury bond ETF, MBB is a mortgage backed security ETF,
#### USO is an oil and gas commodity ETF, DBB is a metals commodity ETF, DBA is an agriculture commodity ETF,
#### VNQ is a real estate ETF, SH is an inverse equities ETF, EUO is a leveraged currencies ETF, FGD is a global equities ETF
#### This is an aggresive portfolio with an inverse ETF as a hedge

mystocks = c("SPY", "SHY", "MBB", "USO", "DBB", "DBA", "VNQ", "SH", "EUO", "FGD")
myprices = getSymbols(mystocks, from = "2014-01-01")


#### Adjusting Stocks
for(ticker in mystocks) {
  expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
  eval(parse(text=expr))
}

# Combine all the returns in a matrix
all_returns = cbind(	ClCl(SPYa),
                     ClCl(SHYa),
                     ClCl(MBBa),
                     ClCl(USOa),
                     ClCl(DBBa),
                     ClCl(DBAa),
                     ClCl(VNQa),
                     ClCl(SHa),
                     ClCl(EUOa),
                     ClCl(FGDa))
all_returns = as.matrix(na.omit(all_returns))

# Compute the returns from the closing prices
pairs(all_returns)

# Sample a random return from the empirical joint distribution
return.today = resample(all_returns, 1, orig.ids=FALSE)

# Update the value of your holdings, assuming equal allocation
total_wealth = 100000
my_weights = c(0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1)
holdings = total_wealth*my_weights
holdings = holdings*(1 + return.today)

# Compute new total wealth
holdings
total_wealth = sum(holdings)
total_wealth


# Now simulate many different possible futures
initial_wealth = 100000
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
  total_wealth = initial_wealth
  weights = c(0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1)
  holdings = weights * total_wealth
  n_days = 20
  wealthtracker = rep(0, n_days)
  for(today in 1:n_days) {
    return.today = resample(all_returns, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
  }
  wealthtracker
}

# each row is a simulated trajectory
# each column is a data
head(sim1)
hist(sim1[,n_days], 25)

# Profit/loss
mean(sim1[,n_days])
mean(sim1[,n_days] - initial_wealth)
hist(sim1[,n_days]- initial_wealth, breaks=30)

# 5% value at risk:
quantile(sim1[,n_days]- initial_wealth, prob=0.05)
```

This is an aggressive portolio with multiple ETF's across bonds, large caps, mid caps, commodities, inverse equities, leveraged currencies and global equities. SPY is a large cap growth ETF, SHY is a short treasury bond ETF, MBB is a mortgage backed security ETF, USO is an oil and gas commodity ETF, DBB is a metals commodity ETF, DBA is an agriculture commodity ETF, VNQ is a real estate ETF, SH is an inverse equities ETF, EUO is a leveraged currencies ETF, FGD is a global equities ETF. Here we see the high risk, high reward strategy paying dividends. We find the highest frequency of returns to be positive, though by a slight margin. Also, it is worthwhile to note here that there is a long tail of very negative returns (high risk).

#Problem 4

The goal of this report is to help NutrientH2O find market segments that may be of interest. For the purposes of this goal, k-means clustering has been used and several different predictors have been plotted to try and understand the market segments present with NutrientH2O's online audience.

```{r loadingdata, echo = FALSE, message=FALSE}
rm(list = ls())
library(ggplot2)
library(LICORS)
library(foreach)
library(mosaic)
library(foreach)
set.seed(52)
social = read.csv('data/social_marketing.csv', header=TRUE)
rownames(social) <- social$X
social = social[,-1]
```

## Cleaning data
It is important to note that the dataset received from Amazon's Mechanical Turk Service has to be cleaned in order to better understand the market. In order to do this any row which had adult or spam content was deleted from the dataset. The next step taken was to filter out rows with high density of uncategorized and chatter content. Upon closer inspection, it was recognized that several rows had high counts in the chatter column. So, it was decided that any row that had a chatter count of more than 2 would be deleted from the dataset along with rows that had content in the uncategorized column.

```{r cleaning, echo =FALSE, message = FALSE}
social = social[!(social$chatter>=2 | social$adult>=1 | social$spam>=1 | social$uncategorized>=1),]
```

## Scaling data
After cleaning the dataset, steps were taken to scale the dataset.In the process of scaling, columns for adult, spam, uncategorized and chatter were deleted. The means and the standard deviations of the dataset were saved so that they could be reverted back to understandable data points once clusters were made.

```{r scaling, echo = FALSE, message = FALSE}
X = social
X = X[,-5]
X = X[,-(34:35)]
X = X[,-1]
X_for_elbow = X
X = scale(X, center=TRUE, scale=TRUE)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")
```

## Choosing a value for k
In order to create the model, a number had to be chosen for the k. The Gap statistic method was first used for this purpose. The plot from the gap statistic method is shown:

#{r gap, echo = FALSE,fig.align='center', fig.height = 5, fig.width = 8,echo = FALSE, message = FALSE, warning = #FALSE}
#library(cluster)
#mark_gap = clusGap(social, FUN = kmeans, nstart = 50, K.max = 20, B = 100)
#plot(mark_gap)

As can be seen, no clear dip could be observed. So, it was decided after several trials that a k value of 7 would be chosen. This was based on formation of discernible clusters for this specific value of k.

## Identifying clusters
At first, a combination of different variables were plotted. One of the first pairs of covariates that showed an interesting cluster was beauty and fashion.

```{r beatyfasion, echo = FALSE,fig.align='center', fig.height = 5, fig.width = 8}
clust1 = kmeanspp(X, 7, nstart=25)
qplot(fashion, beauty, data=social, color=factor(clust1$cluster))
```

The concentration of members of the same cluster for higher number of tweets in fashion and beauty suggests that it is a pretty clear cluster which can considered as an exploitable market.This cluster can be thought as consisting of young individuals who are interested in appearance and fashion trends.

Driven by this finding of clusters formed by individuals who were interested in fashion and beauty, another plot was created with the covariates of travel and personal_fitness.

```{r travelfit, echo = FALSE,fig.align='center', fig.height = 5, fig.width = 8}
qplot(travel, personal_fitness, data=social, color=factor(clust1$cluster))
```

In the plot above, two other clusters emerge: one of people who are very interested in personal fitness and another group for individuals who are very interested in travel. In order to see if the cluster of people interested in personal fitness were in someway also interested in playing sports, another plot was created.

```{r fitsport, echo = FALSE,fig.align='center', fig.height = 5, fig.width = 8}
qplot(sports_playing, personal_fitness, data=social, color=factor(clust1$cluster))
```
The above plot suggests that the hypothesis of the existence of a market segment that is interested in both playing sports and fitness is invalid. However, in order to further investigate if the brand's audience included active college students, a plot was created with the variables college_uni and sports_playing. 

```{r sportuni, echo = FALSE,fig.align='center', fig.height = 5, fig.width = 8}
qplot(college_uni, sports_playing, data=social, color=factor(clust1$cluster))
```
The existence of two distinct clusters is clear in the plot above. One with individuals who attend college and have an active lifestyle and another with neither one of those qualities.Since the demographic that goes to college and plays sports consists of young adults, further investigation was conducted into the online gaming community since its membership is dominated to a certain extent by young adults.

```{r gametrave, echo = FALSE,fig.align='center', fig.height = 5, fig.width = 8}
qplot(travel, online_gaming, data=social, color=factor(clust1$cluster))
```
From the above plot, it is clear that there is cluster of individuals who are heavily involved in online gaming and are interested in NutrientH2O.In an effort to see if there exists clusters among tech savvy individuals, the covariates online_gaming and computers were plotted.

```{r componli, echo = FALSE,fig.align='center', fig.height = 5, fig.width = 8}
qplot(online_gaming, computers, data=social, color=factor(clust1$cluster))
```

This shows that there are actually two distinct clusters: a group for online gamers and others who are solely interested in computers. So this could be interpreted as clusters for younger individuals and another for possibly older individuals just interested in computers.

In an effort to understand if individuals who were interested in healthy eating were a target audience, a plot was made using personal_fitness and health_nutrition. As can be seen below, two distinct clusetrs form: one whose membership consists of individuals interested in healthy eating and personal fitness and another who are not interested in either.


```{r healthfit, echo = FALSE,fig.align='center', fig.height = 5, fig.width = 8}
qplot(health_nutrition, personal_fitness, data=social, color=factor(clust1$cluster))
```


It is worth noting that other covariates such as politics, news, family etc. were plotted but informative clusters were not found. 

## Conclusion

From the above analysis it is clear that the brand NutrientH2O appeals to certain groups of the general populace, namely:

-individuals interested in beauty and fashion: possibly the younger side of the population who follow influencers.
-active individuals attending college: a demographic primarily between the ages of 18 and 22.
-fitness obsessed individuals.
-individuals interested in travel.
-online gamers: once again, most likely a younger portion of the population.
-individuals interested in technology.

#Problem 5


#Problem 6
```{r, echo = TRUE}
library(tidyverse)
library(arules)  
library(arulesViz)

# Reading in as a sparse matrix
grocery_list <- read.transactions("data/groceries.txt", sep=',')
summary(grocery_list)

# Looking at items in first ten transactions
arules::inspect(grocery_list[1:10])

# Checking frequencies of first ten items. This will give us an idea on the 'support'
itemFrequency(grocery_list[, 1:10])

# Running the apriori rule
grocery_rules <- apriori(grocery_list, parameter = list(support = 0.003, confidence = 0.25, minlen = 2))
summary(grocery_rules)
```

We see that our data contains 9835 transactions and 169 different items. Also there were 2159 transactions with only one item purchased, 1643 transactions with two items purchased and so on.
Also, item frequencies tell us the support for the first ten products, like the abrasive cleaner has 0.36% support, artificial sweetener has 0.32% support, baby cosmetics have 0.061% support and so on.

```{r, echo = TRUE}

# Looking at a list of first twenty grocery rules, ordered by lift (to find the most interesting ones)
arules::inspect(sort(grocery_rules, by = 'lift')[1:20])

# Depending on the value of lifts in the previous output, generating a subset of rules to plot (cutoff by lift threshold)
int_rules <- subset(grocery_rules, subset= lift > 4.5)

# Visualizing the subset
plot(int_rules, method="graph", control=list(type="items"))
```
Here we have created a graph to see the top rules with the highest 'lifts', which signifies that these rules can probably be insightful as they happen frequently and not at random. Here are some interesting trends:
1. Customers who bought instant food products are 12 times more likely to buy hamburger meat!
2. Customers who bought flour are 9 times more likely to buy sugar. Seems like flour relates to baking more often than not!
3. Customers who bought processed cheese are 6 times more likely to buy white bread. Looks like these go well together!
4. An interesting one is someone buying onions and whole milk is 5 times more likely to buy butter. Speaking from personal experience, this could relate to the similar frequency of usage of these items in cooking!
5. Another interesting one is someone who picks beef and soda is nearly 5 times more likely to pick root vegetables. Looks like an European cuisine of meat and veggies with soda accompaniment is popular in the US!
```{r}

# Visualizing on the value of support and lift, based on confidence
plotly_arules(grocery_rules, measure = c("support", "lift"), shading = "confidence")

```
The plot gives an overview of the distribution of support and lift in the grocery rule set. There are a few high-lift rules, located close to the minimum support threshold. This means that there are some items which might not occur frequently but form strong rules with some other products, according to the apriori principle.

```{r, echo=TRUE}
plotly_arules(grocery_rules, method = "two-key plot")

```
Here we see a plot between support and confidence. We find a lot of points with high confidence and support in the bottom left corner, extending upwards on both the x and the y axes. This means that there are items which occur frequently in the grocery list and they also form strong rules with other group of items as per the apriori principle.

```{r, echo = TRUE}

# Visualizing frequency of top 10 support items
itemFrequencyPlot(grocery_list, topN = 10)

# For each item in the previous list, building a subset of rules

# inspecting rules for 'whole milk'
ham_rules <- subset(grocery_rules, items %in% 'ham')
arules::inspect(ham_rules[1:10])

# inspecting rules for 'other vegetables'
veg_rules <- subset(grocery_rules, items %in% 'other vegetables')
arules::inspect(veg_rules[1:10])

# inspecting rules for 'rolls/buns'
bun_rules <- subset(grocery_rules, items %in% 'rolls/buns')
arules::inspect(bun_rules[1:10])

# inspecting rules for 'soda'
soda_rules <- subset(grocery_rules, items %in% 'soda')
arules::inspect(soda_rules[1:10])

# inspecting rules for 'yogurt'
yogurt_rules <- subset(grocery_rules, items %in% 'yogurt')
arules::inspect(yogurt_rules[1:10])

# inspecting rules for 'bottled water'
water_rules <- subset(grocery_rules, items %in% 'bottled water')
arules::inspect(water_rules[1:10])

# inspecting rules for 'root vegetables'
root_rules <- subset(grocery_rules, items %in% 'root vegetables')
arules::inspect(root_rules[1:10])

# inspecting rules for 'tropical fruit'
fruit_rules <- subset(grocery_rules, items %in% 'tropical fruit')
arules::inspect(fruit_rules[1:10])

# inspecting rules for 'shopping bags'
bag_rules <- subset(grocery_rules, items %in% 'shopping bags')
arules::inspect(bag_rules[1:10])

# inspecting rules for 'sausage'
sausage_rules <- subset(grocery_rules, items %in% 'sausage')
arules::inspect(sausage_rules[1:10])

```

Going through this list, we find some interesting rules such as:
1.Customers who buy speciality cheese are 3 times more likely to buy other vegetables.
2.Customers who buy herbs and other vegetables are 5 times more likely to buy root vegetables. This seems counter-intuitive since these customrs have already bought vegetables! Interesting insight.
3.Customers who bought baking power and whole milk are 3 times more likely to buy other vegetables. This insight can dictate where to place the vegetables relative to the baking and the dairy section in a supermart!
4.Customers who buy hard cheese and other veg are 4 times more likely to pick root vegetables, building on our hunch of placing the dairy section right next to the vegetable section.
5.Customers who pick butter, other vegetables and yoghurt are 5 times more likely to pick tropical fruits (not citrus fruits). This insight could be useful in placing fruits in a supermart and increasing sales on a perishable item!




